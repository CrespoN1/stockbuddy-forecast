{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 06 â€” Model Evaluation & Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from src.evaluation import *\n",
    "from src.baselines import *\n",
    "from src.forecasters import *\n",
    "from src.data_loader import fetch_stock_data\n",
    "\n",
    "# Configure visualization\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data & Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load stock data for evaluation\n",
    "sample_tickers = ['AAPL', 'MSFT', 'GOOGL']  # Demo stocks\n",
    "data_dict = {}\n",
    "\n",
    "for ticker in sample_tickers:\n",
    "    try:\n",
    "        data = fetch_stock_data(ticker, start_date='2022-01-01', end_date='2024-12-31')\n",
    "        data_dict[ticker] = data\n",
    "        print(f\"Loaded {ticker}: {len(data)} records\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {ticker}: {e}\")\n",
    "\n",
    "print(f\"\\nLoaded data for {len(data_dict)} stocks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Walk-Forward Validation Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate walk_forward_split on a sample stock\n",
    "sample_ticker = sample_tickers[0]\n",
    "sample_data = data_dict[sample_ticker]\n",
    "\n",
    "# Parameters for walk-forward validation\n",
    "train_size = 500\n",
    "test_size = 60\n",
    "step_size = 30\n",
    "\n",
    "splits = walk_forward_split(sample_data, train_size=train_size, test_size=test_size, step_size=step_size)\n",
    "\n",
    "print(f\"Number of walk-forward splits: {len(splits)}\")\n",
    "print(f\"\\nFirst split details:\")\n",
    "for i, (train_idx, test_idx) in enumerate(splits[:3]):\n",
    "    print(f\"  Split {i+1}: train={len(train_idx)}, test={len(test_idx)}\")\n",
    "\n",
    "# Visualize the train/test windows\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "for i, (train_idx, test_idx) in enumerate(splits[:5]):\n",
    "    ax.axvspan(min(train_idx), max(train_idx), alpha=0.2, color='blue', label='Train' if i == 0 else '')\n",
    "    ax.axvspan(min(test_idx), max(test_idx), alpha=0.2, color='red', label='Test' if i == 0 else '')\n",
    "\n",
    "ax.set_xlabel('Sample Index')\n",
    "ax.set_ylabel('Split Number')\n",
    "ax.set_title(f'Walk-Forward Validation Windows for {sample_ticker}')\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nWalk-forward validation setup complete with {len(splits)} splits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Backtest All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_baseline_backtest(data_dict, model_classes, horizons=[1, 5, 10]):\n",
    "    \"\"\"\n",
    "    Run backtesting for each model class on demo stocks.\n",
    "    Returns a dictionary of results DataFrames, one per model.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    for model_name, model_class in model_classes.items():\n",
    "        print(f\"\\nBacktesting {model_name}...\")\n",
    "        model_results = []\n",
    "        \n",
    "        for ticker, data in data_dict.items():\n",
    "            print(f\"  {ticker}...\", end=' ')\n",
    "            \n",
    "            try:\n",
    "                # Initialize model\n",
    "                model = model_class()\n",
    "                \n",
    "                # Run backtesting with walk-forward validation\n",
    "                train_size = 500\n",
    "                test_size = 60\n",
    "                step_size = 30\n",
    "                \n",
    "                splits = walk_forward_split(data, train_size=train_size, test_size=test_size, step_size=step_size)\n",
    "                \n",
    "                # Collect metrics for each split and horizon\n",
    "                for split_idx, (train_idx, test_idx) in enumerate(splits[:3]):  # Limit to 3 splits for demo\n",
    "                    train_data = data.iloc[train_idx]\n",
    "                    test_data = data.iloc[test_idx]\n",
    "                    \n",
    "                    for horizon in horizons:\n",
    "                        if len(test_data) >= horizon:\n",
    "                            try:\n",
    "                                # Fit and forecast\n",
    "                                model.fit(train_data['Close'].values)\n",
    "                                forecast = model.forecast(horizon=horizon)\n",
    "                                actual = test_data['Close'].values[:horizon]\n",
    "                                \n",
    "                                # Calculate metrics\n",
    "                                rmse = np.sqrt(np.mean((forecast - actual) ** 2))\n",
    "                                mae = np.mean(np.abs(forecast - actual))\n",
    "                                mape = np.mean(np.abs((forecast - actual) / actual)) * 100\n",
    "                                \n",
    "                                # Directional accuracy\n",
    "                                actual_direction = np.sign(np.diff(actual))\n",
    "                                forecast_direction = np.sign(np.diff(forecast))\n",
    "                                da = np.mean(actual_direction == forecast_direction) * 100\n",
    "                                \n",
    "                                model_results.append({\n",
    "                                    'Ticker': ticker,\n",
    "                                    'Split': split_idx,\n",
    "                                    'Horizon': horizon,\n",
    "                                    'RMSE': rmse,\n",
    "                                    'MAE': mae,\n",
    "                                    'MAPE': mape,\n",
    "                                    'DA': da\n",
    "                                })\n",
    "                            except Exception as e:\n",
    "                                print(f\"Error in forecast: {e}\", end=' ')\n",
    "                print(\"OK\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error: {e}\")\n",
    "        \n",
    "        results[model_name] = pd.DataFrame(model_results)\n",
    "        print(f\"  {model_name}: {len(model_results)} results\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Define baseline models to backtest\n",
    "baseline_models = {\n",
    "    'Naive': NaiveForecast,\n",
    "    'RandomWalk': RandomWalkForecast,\n",
    "    'SMA': SMAForecast,\n",
    "    'ARIMA': ARIMAForecast\n",
    "}\n",
    "\n",
    "# Run backtesting\n",
    "backtest_results = run_baseline_backtest(data_dict, baseline_models)\n",
    "print(\"\\nBaseline backtesting complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Baseline Backtest Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display backtest results for each model\n",
    "for model_name, results_df in backtest_results.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"{model_name} - Backtest Results\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(results_df.head(10))\n",
    "    \n",
    "    # Show mean metrics per horizon\n",
    "    print(f\"\\nMean metrics by horizon:\")\n",
    "    horizon_summary = results_df.groupby('Horizon')[['RMSE', 'MAE', 'MAPE', 'DA']].mean()\n",
    "    print(horizon_summary)\n",
    "\n",
    "# Combined comparison table\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"Overall Mean Performance by Model\")\n",
    "print(f\"{'='*60}\")\n",
    "overall_means = {}\n",
    "for model_name, results_df in backtest_results.items():\n",
    "    overall_means[model_name] = {\n",
    "        'RMSE': results_df['RMSE'].mean(),\n",
    "        'MAE': results_df['MAE'].mean(),\n",
    "        'MAPE': results_df['MAPE'].mean(),\n",
    "        'DA': results_df['DA'].mean()\n",
    "    }\n",
    "\n",
    "overall_df = pd.DataFrame(overall_means).T\n",
    "print(overall_df.round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Advanced Model Backtesting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attempt to backtest LSTM if TensorFlow is available\n",
    "try:\n",
    "    from src.forecasters import LSTMForecast\n",
    "    import tensorflow as tf\n",
    "    \n",
    "    print(\"TensorFlow available. Backtesting LSTM...\")\n",
    "    \n",
    "    lstm_results = []\n",
    "    for ticker in sample_tickers[:1]:  # Test on one stock for speed\n",
    "        data = data_dict[ticker]\n",
    "        print(f\"\\nLSTM backtest for {ticker}...\")\n",
    "        \n",
    "        try:\n",
    "            model = LSTMForecast(units=64, epochs=10, batch_size=32)\n",
    "            \n",
    "            train_size = 500\n",
    "            test_size = 60\n",
    "            step_size = 30\n",
    "            splits = walk_forward_split(data, train_size=train_size, test_size=test_size, step_size=step_size)\n",
    "            \n",
    "            for split_idx, (train_idx, test_idx) in enumerate(splits[:1]):  # One split for demo\n",
    "                train_data = data.iloc[train_idx]\n",
    "                test_data = data.iloc[test_idx]\n",
    "                \n",
    "                for horizon in [1, 5]:\n",
    "                    try:\n",
    "                        model.fit(train_data['Close'].values)\n",
    "                        forecast = model.forecast(horizon=horizon)\n",
    "                        actual = test_data['Close'].values[:horizon]\n",
    "                        \n",
    "                        rmse = np.sqrt(np.mean((forecast - actual) ** 2))\n",
    "                        mae = np.mean(np.abs(forecast - actual))\n",
    "                        mape = np.mean(np.abs((forecast - actual) / actual)) * 100\n",
    "                        actual_direction = np.sign(np.diff(actual))\n",
    "                        forecast_direction = np.sign(np.diff(forecast))\n",
    "                        da = np.mean(actual_direction == forecast_direction) * 100 if len(actual_direction) > 0 else 0\n",
    "                        \n",
    "                        lstm_results.append({\n",
    "                            'Ticker': ticker,\n",
    "                            'Split': split_idx,\n",
    "                            'Horizon': horizon,\n",
    "                            'RMSE': rmse,\n",
    "                            'MAE': mae,\n",
    "                            'MAPE': mape,\n",
    "                            'DA': da\n",
    "                        })\n",
    "                    except Exception as e:\n",
    "                        print(f\"  LSTM forecast error: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  LSTM model error: {e}\")\n",
    "    \n",
    "    if lstm_results:\n",
    "        backtest_results['LSTM'] = pd.DataFrame(lstm_results)\n",
    "        print(f\"\\nLSTM backtest complete: {len(lstm_results)} results\")\n",
    "    else:\n",
    "        print(\"No LSTM results generated\")\n",
    "\n",
    "except ImportError as e:\n",
    "    print(f\"TensorFlow not available: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during LSTM backtesting: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_models(results_dict):\n",
    "    \"\"\"\n",
    "    Create a comprehensive comparison summary of all models.\n",
    "    \"\"\"\n",
    "    comparison_data = []\n",
    "    \n",
    "    for model_name, results_df in results_dict.items():\n",
    "        # Overall metrics\n",
    "        summary = {\n",
    "            'Model': model_name,\n",
    "            'RMSE_mean': results_df['RMSE'].mean(),\n",
    "            'RMSE_std': results_df['RMSE'].std(),\n",
    "            'MAE_mean': results_df['MAE'].mean(),\n",
    "            'MAE_std': results_df['MAE'].std(),\n",
    "            'MAPE_mean': results_df['MAPE'].mean(),\n",
    "            'MAPE_std': results_df['MAPE'].std(),\n",
    "            'DA_mean': results_df['DA'].mean(),\n",
    "            'DA_std': results_df['DA'].std(),\n",
    "            'N_Results': len(results_df)\n",
    "        }\n",
    "        comparison_data.append(summary)\n",
    "    \n",
    "    comparison_df = pd.DataFrame(comparison_data)\n",
    "    return comparison_df.sort_values('RMSE_mean')\n",
    "\n",
    "# Generate comparison summary\n",
    "summary = compare_models(backtest_results)\n",
    "print(\"Model Comparison Summary\")\n",
    "print(\"=\"*100)\n",
    "print(summary.to_string(index=False))\n",
    "print(\"\\nRanking: Models sorted by mean RMSE (lower is better)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Comparison Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model_comparison_heatmap(summary_df, metric='RMSE_mean', save=False):\n",
    "    \"\"\"\n",
    "    Plot a heatmap of model performance across metrics.\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    # Prepare data for heatmap\n",
    "    metric_cols = [col for col in summary_df.columns if '_mean' in col]\n",
    "    heatmap_data = summary_df.set_index('Model')[metric_cols].copy()\n",
    "    \n",
    "    # Normalize for visualization\n",
    "    heatmap_data_norm = (heatmap_data - heatmap_data.min()) / (heatmap_data.max() - heatmap_data.min())\n",
    "    \n",
    "    # Create heatmap\n",
    "    sns.heatmap(heatmap_data_norm, annot=heatmap_data.round(3), fmt='g', cmap='RdYlGn_r',\n",
    "                cbar_kws={'label': 'Normalized Score (lower is better)'}, ax=ax)\n",
    "    ax.set_title('Model Performance Comparison Heatmap')\n",
    "    ax.set_ylabel('Model')\n",
    "    ax.set_xlabel('Metric')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    if save:\n",
    "        plt.savefig('../figures/model_comparison_heatmap.png', dpi=300, bbox_inches='tight')\n",
    "        print(\"Saved: ../figures/model_comparison_heatmap.png\")\n",
    "    plt.show()\n",
    "\n",
    "# Plot heatmap\n",
    "plot_model_comparison_heatmap(summary, save=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Metrics Bar Charts"
   ]
  },
  {
   "cell_type": "code">
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics_bar_chart(results_dict, metric='RMSE', save=False):\n",
    "    \"\"\"\n",
    "    Create bar charts comparing model performance across horizons.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    metrics = ['RMSE', 'MAE', 'MAPE', 'DA']\n",
    "    \n",
    "    for ax, metric_name in zip(axes.flat, metrics):\n",
    "        # Collect data by model and horizon\n",
    "        plot_data = []\n",
    "        for model_name, results_df in results_dict.items():\n",
    "            horizons = sorted(results_df['Horizon'].unique())\n",
    "            for horizon in horizons:\n",
    "                subset = results_df[results_df['Horizon'] == horizon]\n",
    "                mean_val = subset[metric_name].mean()\n",
    "                plot_data.append({\n",
    "                    'Model': model_name,\n",
    "                    'Horizon': f'H={horizon}',\n",
    "                    'Value': mean_val\n",
    "                })\n",
    "        \n",
    "        plot_df = pd.DataFrame(plot_data)\n",
    "        \n",
    "        # Create grouped bar chart\n",
    "        horizons_list = sorted(plot_df['Horizon'].unique())\n",
    "        models_list = sorted(plot_df['Model'].unique())\n",
    "        \n",
    "        x = np.arange(len(horizons_list))\n",
    "        width = 0.2\n",
    "        \n",
    "        for i, model in enumerate(models_list):\n",
    "            model_data = plot_df[plot_df['Model'] == model].sort_values('Horizon')\n",
    "            ax.bar(x + i*width, model_data['Value'].values, width, label=model)\n",
    "        \n",
    "        ax.set_xlabel('Forecast Horizon')\n",
    "        ax.set_ylabel(metric_name)\n",
    "        ax.set_title(f'{metric_name} by Model and Horizon')\n",
    "        ax.set_xticks(x + width * (len(models_list) - 1) / 2)\n",
    "        ax.set_xticklabels(horizons_list)\n",
    "        ax.legend()\n",
    "        ax.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    if save:\n",
    "        plt.savefig('../figures/metrics_bar_charts.png', dpi=300, bbox_inches='tight')\n",
    "        print(\"Saved: ../figures/metrics_bar_charts.png\")\n",
    "    plt.show()\n",
    "\n",
    "# Plot bar charts for all metrics\n",
    "plot_metrics_bar_chart(backtest_results, save=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Statistical Significance Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "def diebold_mariano_test(forecast1, forecast2, actual, horizon=1):\n",
    "    \"\"\"\n",
    "    Simplified Diebold-Mariano style test comparing two forecast series.\n",
    "    Returns p-value for whether forecasts are significantly different.\n",
    "    \"\"\"\n",
    "    error1 = forecast1 - actual\n",
    "    error2 = forecast2 - actual\n",
    "    \n",
    "    # Loss differential\n",
    "    d = (error1 ** 2) - (error2 ** 2)\n",
    "    \n",
    "    # t-test on loss differentials\n",
    "    if len(d) > 1 and d.std() > 0:\n",
    "        t_stat = d.mean() / (d.std() / np.sqrt(len(d)))\n",
    "        p_value = 2 * (1 - stats.t.cdf(np.abs(t_stat), len(d) - 1))\n",
    "        return p_value\n",
    "    return np.nan\n",
    "\n",
    "# Run significance tests between best model and baselines\n",
    "print(\"Statistical Significance Tests (Diebold-Mariano style)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Find best model by RMSE\n",
    "best_model = summary.iloc[0]['Model']\n",
    "print(f\"\\nBest model: {best_model}\")\n",
    "print(f\"\\nTesting {best_model} vs. other models...\\n\")\n",
    "\n",
    "# Create comparison table\n",
    "sig_tests = []\n",
    "best_results = backtest_results[best_model]\n",
    "\n",
    "for model_name, results_df in backtest_results.items():\n",
    "    if model_name == best_model:\n",
    "        continue\n",
    "    \n",
    "    # Collect test results\n",
    "    p_values = []\n",
    "    for horizon in sorted(results_df['Horizon'].unique()):\n",
    "        best_h = best_results[best_results['Horizon'] == horizon]['RMSE'].values\n",
    "        other_h = results_df[results_df['Horizon'] == horizon]['RMSE'].values\n",
    "        \n",
    "        if len(best_h) > 0 and len(other_h) > 0:\n",
    "            # Simple comparison: t-test on RMSE\n",
    "            t_stat, p_val = stats.ttest_ind(best_h, other_h)\n",
    "            p_values.append(p_val)\n",
    "    \n",
    "    mean_p = np.mean(p_values) if p_values else np.nan\n",
    "    sig_tests.append({\n",
    "        'Model': model_name,\n",
    "        'vs. Best': best_model,\n",
    "        'Mean_P_Value': mean_p,\n",
    "        'Significant': 'Yes' if mean_p < 0.05 else 'No'\n",
    "    })\n",
    "\n",
    "sig_df = pd.DataFrame(sig_tests)\n",
    "print(sig_df.to_string(index=False))\n",
    "print(\"\\nNote: P-value < 0.05 indicates statistically significant difference\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Cluster-Level Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster-level analysis (if cluster data available)\n",
    "try:\n",
    "    from src.clustering import load_cluster_assignments\n",
    "    \n",
    "    print(\"Performing cluster-level analysis...\\n\")\n",
    "    \n",
    "    # Load cluster assignments\n",
    "    cluster_map = load_cluster_assignments()  # Assume this returns dict of ticker -> cluster\n",
    "    \n",
    "    # Group results by cluster\n",
    "    cluster_analysis = {}\n",
    "    \n",
    "    for model_name, results_df in backtest_results.items():\n",
    "        # Add cluster information\n",
    "        results_df['Cluster'] = results_df['Ticker'].map(cluster_map)\n",
    "        \n",
    "        # Group by cluster\n",
    "        cluster_performance = results_df.groupby('Cluster')[['RMSE', 'MAE', 'MAPE', 'DA']].mean()\n",
    "        cluster_analysis[model_name] = cluster_performance\n",
    "        \n",
    "        print(f\"\\n{model_name} - Performance by Cluster:\")\n",
    "        print(cluster_performance.round(3))\n",
    "    \n",
    "    # Analyze if cluster-informed models perform better in certain clusters\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"Cluster-Specific Model Performance\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    for cluster_id in sorted(set(cluster_map.values())):\n",
    "        print(f\"\\nCluster {cluster_id}:\")\n",
    "        for model_name in cluster_analysis:\n",
    "            if cluster_id in cluster_analysis[model_name].index:\n",
    "                rmse = cluster_analysis[model_name].loc[cluster_id, 'RMSE']\n",
    "                print(f\"  {model_name}: RMSE = {rmse:.4f}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Cluster analysis not available: {e}\")\n",
    "    print(\"Skipping cluster-level analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Key Findings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of Evaluation Results\n",
    "\n",
    "**Best Performing Models:**\n",
    "- The model comparison above shows ranking by mean RMSE (lower is better)\n",
    "- Top-performing model provides the lowest prediction error across all stocks and horizons\n",
    "\n",
    "**Performance by Forecast Horizon:**\n",
    "- Shorter horizons (H=1) typically have lower error rates\n",
    "- Performance degrades as we forecast further into the future\n",
    "- This is expected due to increasing uncertainty\n",
    "\n",
    "**Directional Accuracy:**\n",
    "- Directional accuracy (DA) measures ability to predict whether price will go up or down\n",
    "- Some models may have high DA but lower point accuracy\n",
    "- This suggests value in predicting direction even if magnitude is off\n",
    "\n",
    "**Statistical Significance:**\n",
    "- Results table above shows p-values from statistical tests\n",
    "- P-value < 0.05 indicates statistically significant difference between models\n",
    "- This helps identify which model improvements are real vs. due to random variation\n",
    "\n",
    "**Cluster Insights:**\n",
    "- Some models may perform better on specific stock clusters\n",
    "- This could suggest leveraging cluster-aware model selection\n",
    "- Consider cluster characteristics when deploying models in production\n",
    "\n",
    "**Recommendations:**\n",
    "1. Use ensemble of top-performing models for robustness\n",
    "2. Consider directional forecasting for trading strategies\n",
    "3. Shorter horizons more reliable for portfolio decisions\n",
    "4. Monitor performance on new data; retrain if accuracy degrades\n",
    "5. Consider cluster-specific model deployment if insights are strong"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
