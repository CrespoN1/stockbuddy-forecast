{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 06 \u2014 Model Evaluation & Backtesting\n",
    "Systematic evaluation using walk-forward validation across multiple horizons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "\n",
    "from src.data_loader import fetch_multiple_stocks\n",
    "from src.baselines import NaiveForecast, RandomWalkForecast, SMAForecast, ARIMAForecast\n",
    "from src.evaluation import backtest_model, compare_models, walk_forward_split, compute_all_metrics, plot_model_comparison_heatmap, plot_metrics_bar_chart, statistical_test\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SELECTED_TICKERS = [\n",
    "    'AAPL', 'MSFT', 'GOOGL', 'AMZN', 'NVDA',\n",
    "    'META', 'TESLA', 'BERKB', 'JPM', 'JNJ',\n",
    "    'XOM', 'WMT', 'PG', 'MA', 'V',\n",
    "    'HD', 'DIS', 'PYPL', 'NFLX', 'TSLA',\n",
    "    'ADBE', 'CRM', 'INTC', 'AMD', 'CSCO',\n",
    "    'IBM', 'BA', 'GE', 'CAT', 'MMM'\n",
    "]\n",
    "\n",
    "stock_data = fetch_multiple_stocks(SELECTED_TICKERS, period=\"2y\")\n",
    "sample_ticker = \"AAPL\"\n",
    "series = stock_data[sample_ticker][\"Close\"]\n",
    "\n",
    "print(f\"Stock data shape: {series.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Walk-Forward Validation Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = walk_forward_split(series, n_windows=5)\n",
    "\n",
    "print(f\"Number of splits: {len(splits)}\")\n",
    "for i, (train_idx, test_idx) in enumerate(splits):\n",
    "    train_dates = series.index[train_idx]\n",
    "    test_dates = series.index[test_idx]\n",
    "    print(f\"Split {i}: Train [{train_dates[0]} to {train_dates[-1]}], Test [{test_dates[0]} to {test_dates[-1]}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Backtest All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_configs = {\n",
    "    \"Naive\": (NaiveForecast, {}),\n",
    "    \"Random Walk\": (RandomWalkForecast, {}),\n",
    "    \"SMA(20)\": (SMAForecast, {\"window\": 20}),\n",
    "    \"ARIMA\": (ARIMAForecast, {})\n",
    "}\n",
    "\n",
    "backtest_results = {}\n",
    "\n",
    "for model_name, (ModelClass, kwargs) in model_configs.items():\n",
    "    model = ModelClass(**kwargs)\n",
    "    result = backtest_model(model, series, walk_forward_split(series, n_windows=5))\n",
    "    backtest_results[model_name] = result\n",
    "    print(f\"{model_name}: result shape {result.shape if hasattr(result, 'shape') else len(result)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Compare Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = compare_models(backtest_results)\n",
    "print(summary.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualize Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    plot_model_comparison_heatmap(summary, metric=\"RMSE_mean\", save=True, filename=\"06_model_comparison_rmse.png\")\n",
    "    plot_model_comparison_heatmap(summary, metric=\"MAPE_mean\", save=True, filename=\"06_model_comparison_mape.png\")\n",
    "except Exception as e:\n",
    "    print(f\"Error creating heatmaps: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Metrics Bar Charts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for horizon in [\"1d\", \"1w\", \"1m\"]:\n",
    "    try:\n",
    "        plot_metrics_bar_chart(summary, horizon=horizon, save=True, filename=f\"06_metrics_{horizon}.png\")\n",
    "    except Exception as e:\n",
    "        print(f\"Skipping {horizon}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Statistical Significance Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pairs = [('Naive', 'ARIMA'), ('Random Walk', 'ARIMA'), ('SMA(20)', 'ARIMA')]\n",
    "stat_results = []\n",
    "\n",
    "for model1, model2 in model_pairs:\n",
    "    try:\n",
    "        result = statistical_test(backtest_results[model1], backtest_results[model2])\n",
    "        result['Model 1'] = model1\n",
    "        result['Model 2'] = model2\n",
    "        stat_results.append(result)\n",
    "    except Exception as e:\n",
    "        print(f\"Error comparing {model1} vs {model2}: {e}\")\n",
    "\n",
    "if stat_results:\n",
    "    stat_df = pd.DataFrame(stat_results)\n",
    "    print(stat_df.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Multi-Stock Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_ticker_results = []\n",
    "eval_tickers = ['AAPL', 'MSFT', 'GOOGL', 'AMZN', 'NVDA']\n",
    "\n",
    "for ticker in eval_tickers:\n",
    "    series = stock_data[ticker][\"Close\"]\n",
    "    \n",
    "    naive = NaiveForecast()\n",
    "    naive_result = backtest_model(naive, series, walk_forward_split(series, n_windows=5))\n",
    "    \n",
    "    arima = ARIMAForecast()\n",
    "    arima_result = backtest_model(arima, series, walk_forward_split(series, n_windows=5))\n",
    "    \n",
    "    multi_ticker_results.append({\n",
    "        'ticker': ticker,\n",
    "        'naive_rmse': naive_result.get('RMSE', np.nan) if isinstance(naive_result, dict) else np.nan,\n",
    "        'arima_rmse': arima_result.get('RMSE', np.nan) if isinstance(arima_result, dict) else np.nan\n",
    "    })\n",
    "\n",
    "if multi_ticker_results:\n",
    "    multi_ticker_df = pd.DataFrame(multi_ticker_results)\n",
    "    print(multi_ticker_df.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save Evaluation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary.to_parquet(\"../data/processed/evaluation_summary.parquet\")\n",
    "if multi_ticker_results:\n",
    "    multi_ticker_df.to_parquet(\"../data/processed/multi_ticker_evaluation.parquet\")\n",
    "\n",
    "print(\"Evaluation results saved.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}