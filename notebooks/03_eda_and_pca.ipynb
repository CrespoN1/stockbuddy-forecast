{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 — Exploratory Data Analysis & PCA\n",
    "\n",
    "This notebook performs exploratory data analysis and Principal Component Analysis (PCA) on the engineered features. We will:\n",
    "1. Load the feature matrix from notebook 02\n",
    "2. Analyze feature distributions and identify outliers\n",
    "3. Examine correlation structure\n",
    "4. Apply PCA to reduce dimensionality\n",
    "5. Analyze explained variance\n",
    "6. Interpret PCA loadings\n",
    "7. Visualize stocks in PCA space by sector\n",
    "8. Summarize key findings and save PCA-transformed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../src')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pca_analysis import StockPCA\n",
    "from feature_engineering import create_feature_matrix\n",
    "\n",
    "# Display and plot settings\n",
    "pd.set_option('display.max_columns', 100)\n",
    "sns.set_style('whitegrid')\n",
    "sns.set_palette('husl')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Feature Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load feature matrix from previous notebook\n",
    "try:\n",
    "    feature_matrix = pd.read_parquet('../data/processed/feature_matrix.parquet')\n",
    "    feature_metadata = pd.read_csv('../data/processed/feature_metadata.csv')\n",
    "    print(\"Feature matrix loaded successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading feature matrix: {e}\")\n",
    "    print(\"Please run notebooks 01 and 02 first.\")\n",
    "\n",
    "print(f\"\\nFeature matrix shape: {feature_matrix.shape}\")\n",
    "print(f\"Tickers: {feature_matrix.index.tolist()[:10]}...\")\n",
    "print(f\"\\nFeature matrix info:\")\n",
    "print(feature_matrix.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select numeric features\n",
    "numeric_features = feature_matrix.select_dtypes(include=[np.number])\n",
    "\n",
    "# Plot distributions of a few key features\n",
    "key_features = numeric_features.columns[:8]\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, feature in enumerate(key_features):\n",
    "    data = numeric_features[feature].dropna()\n",
    "    axes[idx].hist(data, bins=30, edgecolor='black', alpha=0.7, color='steelblue')\n",
    "    axes[idx].set_title(f'{feature}', fontweight='bold')\n",
    "    axes[idx].set_ylabel('Frequency')\n",
    "    axes[idx].grid(alpha=0.3)\n",
    "    \n",
    "    mean_val = data.mean()\n",
    "    std_val = data.std()\n",
    "    axes[idx].axvline(mean_val, color='red', linestyle='--', linewidth=2, label=f'Mean: {mean_val:.2f}')\n",
    "    axes[idx].axvline(mean_val - std_val, color='orange', linestyle=':', linewidth=1, label=f'±1 Std')\n",
    "    axes[idx].axvline(mean_val + std_val, color='orange', linestyle=':', linewidth=1)\n",
    "    axes[idx].legend(fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Check for outliers\n",
    "print(f\"\\nOutlier Detection (using IQR method):\")\n",
    "for feature in key_features:\n",
    "    Q1 = numeric_features[feature].quantile(0.25)\n",
    "    Q3 = numeric_features[feature].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    outlier_count = ((numeric_features[feature] < lower_bound) | (numeric_features[feature] > upper_bound)).sum()\n",
    "    if outlier_count > 0:\n",
    "        print(f\"{feature}: {outlier_count} outliers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute correlation matrix\n",
    "correlation_matrix = numeric_features.corr()\n",
    "\n",
    "# Identify highly correlated feature pairs\n",
    "high_corr_pairs = []\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(i+1, len(correlation_matrix.columns)):\n",
    "        if abs(correlation_matrix.iloc[i, j]) > 0.85:\n",
    "            high_corr_pairs.append((\n",
    "                correlation_matrix.columns[i],\n",
    "                correlation_matrix.columns[j],\n",
    "                correlation_matrix.iloc[i, j]\n",
    "            ))\n",
    "\n",
    "# Sort by absolute correlation\n",
    "high_corr_pairs.sort(key=lambda x: abs(x[2]), reverse=True)\n",
    "\n",
    "print(f\"Highly Correlated Feature Pairs (|corr| > 0.85):\")\n",
    "for feat1, feat2, corr in high_corr_pairs[:15]:\n",
    "    print(f\"  {feat1} <-> {feat2}: {corr:.3f}\")\n",
    "\n",
    "# Plot correlation heatmap for a sample of features\n",
    "sample_features = numeric_features.columns[:15]\n",
    "sample_corr = correlation_matrix.loc[sample_features, sample_features]\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(sample_corr, cmap='RdBu_r', center=0, vmin=-1, vmax=1,\n",
    "            square=True, annot=True, fmt='.2f', cbar_kws={'label': 'Correlation'})\n",
    "plt.title('Feature Correlation Matrix (Sample)', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Apply PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize PCA with variance threshold\n",
    "stock_pca = StockPCA(variance_threshold=0.90)\n",
    "\n",
    "# Fit and transform the feature matrix\n",
    "pca_data = stock_pca.fit_transform(numeric_features)\n",
    "\n",
    "print(f\"PCA Analysis Results:\")\n",
    "print(f\"  Original features: {numeric_features.shape[1]}\")\n",
    "print(f\"  PCA components: {pca_data.shape[1]}\")\n",
    "print(f\"  Total variance explained: {stock_pca.explained_variance_ratio_.sum():.4f}\")\n",
    "print(f\"\\nExplained variance by component:\")\n",
    "for i, var in enumerate(stock_pca.explained_variance_ratio_[:10]):\n",
    "    cumsum = stock_pca.explained_variance_ratio_[:i+1].sum()\n",
    "    print(f\"  PC{i+1}: {var:.4f} (Cumulative: {cumsum:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Explained Variance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot explained variance\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Scree plot\n",
    "axes[0].bar(range(1, min(len(stock_pca.explained_variance_ratio_)+1, 21)), \n",
    "            stock_pca.explained_variance_ratio_[:20], alpha=0.7, color='steelblue', edgecolor='black')\n",
    "axes[0].set_xlabel('Principal Component', fontweight='bold')\n",
    "axes[0].set_ylabel('Explained Variance Ratio', fontweight='bold')\n",
    "axes[0].set_title('Scree Plot (Top 20 Components)', fontweight='bold', fontsize=12)\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Cumulative explained variance\n",
    "cumsum_var = np.cumsum(stock_pca.explained_variance_ratio_)\n",
    "axes[1].plot(range(1, min(len(cumsum_var)+1, 31)), cumsum_var[:30], \n",
    "            marker='o', linewidth=2, markersize=6, color='darkgreen')\n",
    "axes[1].axhline(y=0.90, color='red', linestyle='--', linewidth=2, label='90% Variance')\n",
    "axes[1].set_xlabel('Number of Components', fontweight='bold')\n",
    "axes[1].set_ylabel('Cumulative Explained Variance', fontweight='bold')\n",
    "axes[1].set_title('Cumulative Explained Variance', fontweight='bold', fontsize=12)\n",
    "axes[1].grid(alpha=0.3)\n",
    "axes[1].legend()\n",
    "axes[1].set_ylim([0, 1.05])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. PCA Loadings Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get loadings\n",
    "loadings = pd.DataFrame(\n",
    "    stock_pca.components_.T,\n",
    "    columns=[f'PC{i+1}' for i in range(stock_pca.components_.shape[0])],\n",
    "    index=numeric_features.columns\n",
    ")\n",
    "\n",
    "# Display top features for first 3 principal components\n",
    "for pc_num in range(1, 4):\n",
    "    pc_name = f'PC{pc_num}'\n",
    "    top_positive = loadings[pc_name].nlargest(5)\n",
    "    top_negative = loadings[pc_name].nsmallest(5)\n",
    "    \n",
    "    print(f\"\\n{pc_name} Top Features (Explained Variance: {stock_pca.explained_variance_ratio_[pc_num-1]:.4f}):\")\n",
    "    print(f\"  Positive:\")\n",
    "    for feat, val in top_positive.items():\n",
    "        print(f\"    {feat}: {val:.4f}\")\n",
    "    print(f\"  Negative:\")\n",
    "    for feat, val in top_negative.items():\n",
    "        print(f\"    {feat}: {val:.4f}\")\n",
    "\n",
    "# Plot loadings heatmap for first 5 components\n",
    "top_n_features = 20\n",
    "top_var_features = numeric_features.var().nlargest(top_n_features).index\n",
    "top_loadings = loadings.loc[top_var_features, ['PC1', 'PC2', 'PC3', 'PC4', 'PC5']]\n",
    "\n",
    "plt.figure(figsize=(10, 12))\n",
    "sns.heatmap(top_loadings, cmap='RdBu_r', center=0, annot=True, fmt='.2f',\n",
    "            cbar_kws={'label': 'Loading'}, linewidths=0.5)\n",
    "plt.title('PCA Loadings (Top 20 Features by Variance)', fontweight='bold', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Stocks in PCA Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sector mapping (example)\n",
    "sector_mapping = {\n",
    "    'AAPL': 'Technology', 'MSFT': 'Technology', 'GOOGL': 'Technology', 'AMZN': 'Consumer',\n",
    "    'NVDA': 'Technology', 'META': 'Technology', 'AMD': 'Technology', 'INTC': 'Technology',\n",
    "    'TSLA': 'Consumer', 'CRM': 'Technology', 'NFLX': 'Consumer', 'DIS': 'Consumer',\n",
    "    'JPM': 'Financials', 'GS': 'Financials', 'BAC': 'Financials', 'WFC': 'Financials',\n",
    "    'V': 'Financials', 'MA': 'Financials',\n",
    "    'JNJ': 'Healthcare', 'UNH': 'Healthcare', 'LLY': 'Healthcare', 'ABBV': 'Healthcare',\n",
    "    'MRK': 'Healthcare', 'PFE': 'Healthcare',\n",
    "    'XOM': 'Energy', 'CVX': 'Energy', 'COP': 'Energy', 'MPC': 'Energy',\n",
    "    'PG': 'Consumer', 'KO': 'Consumer', 'PEP': 'Consumer', 'WMT': 'Consumer',\n",
    "    'HD': 'Consumer',\n",
    "    'BA': 'Industrials', 'GE': 'Industrials', 'CAT': 'Industrials', 'AVGO': 'Technology'\n",
    "}\n",
    "\n",
    "# Get sector for each ticker in pca_data\n",
    "sectors = pd.Series(\n",
    "    [sector_mapping.get(ticker, 'Other') for ticker in pca_data.index],\n",
    "    index=pca_data.index\n",
    ")\n",
    "\n",
    "# Create 2D scatter plot of stocks in PCA space\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# PC1 vs PC2\n",
    "for sector in sectors.unique():\n",
    "    mask = sectors == sector\n",
    "    axes[0].scatter(pca_data[mask, 0], pca_data[mask, 1], \n",
    "                   s=150, alpha=0.7, label=sector, edgecolors='black', linewidth=1.5)\n",
    "\n",
    "axes[0].set_xlabel(f'PC1 ({stock_pca.explained_variance_ratio_[0]:.2%} variance)', fontweight='bold')\n",
    "axes[0].set_ylabel(f'PC2 ({stock_pca.explained_variance_ratio_[1]:.2%} variance)', fontweight='bold')\n",
    "axes[0].set_title('Stocks in PCA Space (PC1 vs PC2)', fontweight='bold', fontsize=12)\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "axes[0].axhline(y=0, color='k', linestyle='-', linewidth=0.5, alpha=0.5)\n",
    "axes[0].axvline(x=0, color='k', linestyle='-', linewidth=0.5, alpha=0.5)\n",
    "\n",
    "# Add ticker labels\n",
    "for i, ticker in enumerate(pca_data.index):\n",
    "    axes[0].annotate(ticker, (pca_data[i, 0], pca_data[i, 1]), \n",
    "                    fontsize=8, ha='center', va='center')\n",
    "\n",
    "# PC1 vs PC3\n",
    "for sector in sectors.unique():\n",
    "    mask = sectors == sector\n",
    "    axes[1].scatter(pca_data[mask, 0], pca_data[mask, 2], \n",
    "                   s=150, alpha=0.7, label=sector, edgecolors='black', linewidth=1.5)\n",
    "\n",
    "axes[1].set_xlabel(f'PC1 ({stock_pca.explained_variance_ratio_[0]:.2%} variance)', fontweight='bold')\n",
    "axes[1].set_ylabel(f'PC3 ({stock_pca.explained_variance_ratio_[2]:.2%} variance)', fontweight='bold')\n",
    "axes[1].set_title('Stocks in PCA Space (PC1 vs PC3)', fontweight='bold', fontsize=12)\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3)\n",
    "axes[1].axhline(y=0, color='k', linestyle='-', linewidth=0.5, alpha=0.5)\n",
    "axes[1].axvline(x=0, color='k', linestyle='-', linewidth=0.5, alpha=0.5)\n",
    "\n",
    "# Add ticker labels\n",
    "for i, ticker in enumerate(pca_data.index):\n",
    "    axes[1].annotate(ticker, (pca_data[i, 0], pca_data[i, 2]), \n",
    "                    fontsize=8, ha='center', va='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Key Findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of key findings\n",
    "findings = f\"\"\"\n",
    "=== PCA ANALYSIS SUMMARY ===\n",
    "\n",
    "Dimensionality Reduction:\n",
    "  Original feature dimensions: {numeric_features.shape[1]}\n",
    "  Reduced to {pca_data.shape[1]} principal components\n",
    "  Reduction ratio: {(1 - pca_data.shape[1]/numeric_features.shape[1])*100:.1f}%\n",
    "  Variance retained: {stock_pca.explained_variance_ratio_.sum():.2%}\n",
    "\n",
    "Component Importance:\n",
    "  PC1: {stock_pca.explained_variance_ratio_[0]:.2%} of variance\n",
    "  PC2: {stock_pca.explained_variance_ratio_[1]:.2%} of variance\n",
    "  PC3: {stock_pca.explained_variance_ratio_[2]:.2%} of variance\n",
    "  First 5 PCs: {stock_pca.explained_variance_ratio_[:5].sum():.2%} cumulative variance\n",
    "\n",
    "Feature Correlations:\n",
    "  Highly correlated pairs (|r| > 0.85): {len(high_corr_pairs)}\n",
    "  This multicollinearity is handled well by PCA\n",
    "\n",
    "Sector Observations:\n",
    "  Technology stocks show {len(sectors[sectors == 'Technology'])} representatives\n",
    "  Financials stocks show {len(sectors[sectors == 'Financials'])} representatives\n",
    "  Healthcare stocks show {len(sectors[sectors == 'Healthcare'])} representatives\n",
    "\n",
    "Next Steps:\n",
    "  Use PCA-transformed data for clustering analysis\n",
    "  Apply machine learning models with reduced feature set\n",
    "  Validate predictions using original features as well\n",
    "\"\"\"\n",
    "\n",
    "print(findings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Save PCA-transformed data\n",
    "os.makedirs('../data/pca', exist_ok=True)\n",
    "\n",
    "# Create DataFrame with PCA components\n",
    "pca_df = pd.DataFrame(\n",
    "    pca_data,\n",
    "    columns=[f'PC{i+1}' for i in range(pca_data.shape[1])],\n",
    "    index=numeric_features.index\n",
    ")\n",
    "\n",
    "# Save PCA results\n",
    "pca_df.to_parquet('../data/pca/pca_transformed_data.parquet')\n",
    "loadings.to_parquet('../data/pca/pca_loadings.parquet')\n",
    "\n",
    "# Save variance explained\n",
    "variance_df = pd.DataFrame({\n",
    "    'component': [f'PC{i+1}' for i in range(len(stock_pca.explained_variance_ratio_))],\n",
    "    'explained_variance': stock_pca.explained_variance_ratio_,\n",
    "    'cumulative_variance': np.cumsum(stock_pca.explained_variance_ratio_)\n",
    "})\n",
    "variance_df.to_csv('../data/pca/explained_variance.csv', index=False)\n",
    "\n",
    "print(\"PCA results saved successfully!\")\n",
    "print(f\"  - pca_transformed_data.parquet ({pca_df.shape})\")\n",
    "print(f\"  - pca_loadings.parquet ({loadings.shape})\")\n",
    "print(f\"  - explained_variance.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
